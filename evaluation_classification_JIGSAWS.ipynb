{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Device: Tesla P100-PCIE-12GB\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from time import time, sleep\n",
    "\n",
    "from utils import *\n",
    "from jigsaws_utils import *\n",
    "from custom_pca import custom_pca\n",
    "from video_loader import VideoLoader\n",
    "from autoencoders import *\n",
    "\n",
    "seed = 42\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('Device:',torch.cuda.get_device_name(device))\n",
    "\n",
    "def evaluate_model(model, gram_matrix, y, nsplits=5, d_transform=None):\n",
    "    nsample = len(y)\n",
    "    nsample_per_group = int(nsample/nsplits)\n",
    "    repeats = [nsample_per_group]*(nsplits-1)\n",
    "    repeats = repeats + [nsample-np.sum(repeats)]\n",
    "    groups = np.repeat(np.arange(nsplits), repeats)\n",
    "    kfolder = GroupKFold(n_splits=nsplits)\n",
    "\n",
    "    confusion_matrix_cum = np.zeros((ngestures, ngestures))\n",
    "    accuracy_cum = 0\n",
    "    for train_index, test_index in kfolder.split(X, y, groups):\n",
    "        if d_transform is not None:\n",
    "            gram_train = d_transform(gram_matrix[train_index, :][:, train_index])\n",
    "            gram_test = d_transform(gram_matrix[test_index, :][:, train_index])\n",
    "        else:\n",
    "            gram_train = gram_matrix[train_index, :][:, train_index]\n",
    "            gram_test = gram_matrix[test_index, :][:, train_index]\n",
    "\n",
    "        # Predict\n",
    "        model.fit(gram_train, y[train_index])\n",
    "        preds = model.predict(gram_test)\n",
    "        cm = confusion_matrix(y[test_index], preds, normalize='true')\n",
    "\n",
    "        # Handle of missing gestures\n",
    "        if cm.shape != (ngestures,ngestures):\n",
    "            missing_gestures = set(gestures) - set(preds) - set(y[test_index])\n",
    "            l = cm.shape[0]\n",
    "            for g in missing_gestures:\n",
    "                gi = g2i[g]\n",
    "                cm = np.insert(cm, gi, np.zeros(l), axis=1)\n",
    "                cm = np.insert(cm, gi, np.zeros(l+1), axis=0)\n",
    "                cm[gi,gi] = 1\n",
    "                l = l+1\n",
    "\n",
    "        confusion_matrix_cum += cm/nsplits\n",
    "    accuracy = np.mean(np.diag(confusion_matrix_cum))\n",
    "\n",
    "    return accuracy, confusion_matrix_cum\n",
    "\n",
    "def plot_distr(classes, counts, alpha=1):\n",
    "    nclass = len(classes)\n",
    "    x = range(nclass)\n",
    "    plt.bar(x, counts, alpha=alpha)\n",
    "    plt.xticks(x, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in range(3):\n",
    "    _, y = load_video_data(tasks=task)\n",
    "    print(y)\n",
    "    plot_distr(*np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for all tasks and subjects\n",
    "\n",
    "Setup 1: for every task, subject and number of neighbors compute the test accuracy on one fifth of the subject's trials with 5-fold cross-validation. This gives an average accuracy over the fold of each subject. Then average accuracy between the subjects (within the tasks) and plot the average (biased) test accuracy with respect to K.\n",
    "\n",
    "Setup 2: for every task, do as setup 1 except leave one subject out. After applying same procedure as setup 1, pick the best hyperparameter and classify the last subject, which would be the test accuracy.\n",
    "\n",
    "Setup 2 split according to train-validation-test instead of train-test (using test to validate the hyperparameters). But I think setup 1 makes more sense as we are in a experimental framework, not in an evaluation framework.\n",
    "\n",
    "Note: For now, I don't do this, I aggregate all points of a user and run sklearn's corss-validation on it, trials will not be taken into account for the splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,7.4,9])[np.array([True, True, False,False])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knot_Tying:\n",
      "Video not present: task 'Knot_Tying', subject 'B', trial 5, capture 2\n",
      "(74,) 72\n",
      "(72,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:60: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSubject B: 18634 days, 19:55:24\n",
      "(104,) 104\n",
      "(104,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:60: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSubject C: 18634 days, 19:57:01\n",
      "(110,) 110\n",
      "(110,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:60: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSubject D: 18634 days, 19:58:42\n",
      "(108,) 108\n",
      "(108,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:60: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSubject E: 18634 days, 20:00:26\n",
      "(100,) 100\n",
      "(100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:60: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSubject F: 18634 days, 20:02:07\n",
      "(104,) 104\n",
      "(104,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:60: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSubject G: 18634 days, 20:04:10\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'capt' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/work/test/jigsaws_utils.py\u001b[0m in \u001b[0;36mload_video_data\u001b[0;34m(tasks, subjects, trials, captures, gestures)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscr_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/JIGSAWS_converted/Knot_Tying/transcriptions/Knot_Tying_H001.txt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-dcf8aa2e0680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtask_subjects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Load data and fit the systems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mvideo_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_video_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mnsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mgestures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/test/jigsaws_utils.py\u001b[0m in \u001b[0;36mload_video_data\u001b[0;34m(tasks, subjects, trials, captures, gestures)\u001b[0m\n\u001b[1;32m     92\u001b[0m                                 \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgesture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Video not present: task '{task_name}', subject '{subject}', trial {trial}, capture {capt}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'capt' referenced before assignment"
     ]
    }
   ],
   "source": [
    "ks = np.arange(1, 8)\n",
    "gammas = np.logspace(-10, -3, base=2, num=10)\n",
    "ncomp = 10\n",
    "\n",
    "all_confusions_baseline_knn = []\n",
    "all_confusions_baseline_svm = []\n",
    "all_accuracies_baseline_knn  = []\n",
    "all_accuracies_baseline_svm  = []\n",
    "start = time()\n",
    "for task in range(ntask):\n",
    "    all_confusions_baseline_knn.append([])\n",
    "    all_confusions_baseline_svm.append([])\n",
    "    all_accuracies_baseline_knn.append([])\n",
    "    all_accuracies_baseline_svm.append([])\n",
    "    print(i2task[task]+':')\n",
    "    task_dir = 'data/JIGSAWS_converted/'+i2task[task]\n",
    "    stream = os.popen(\"find %s -name '*.avi' | sed 's:^.*_\\([A-Z][0-9]\\{3\\}\\)_[^_]\\+$:\\\\1:'\" % task_dir)\n",
    "    video_meta = list(filter(lambda x: x != '', stream.read().split('\\n')))\n",
    "    task_subjects = np.unique([x[0] for x in video_meta])\n",
    "    for subject in task_subjects:\n",
    "        # Load data and fit the systems\n",
    "        video_data, y = load_video_data(tasks=task, subjects=subject)\n",
    "        nsample = len(y)\n",
    "        gestures = np.unique(y)\n",
    "        ngestures = len(gestures)\n",
    "        g2i = {g: i for i, g in enumerate(gestures)}\n",
    "        X = []\n",
    "        mask_videos = np.full(y.shape, True)\n",
    "        for vi, video in enumerate(video_data):\n",
    "            if video.duration_frames < ncomp:\n",
    "                # Filter too short fragments\n",
    "                mask_videos[vi] = False\n",
    "                continue\n",
    "            video.torch = False\n",
    "            all_frames = video.get_all_frames().reshape(video.duration_frames, -1)\n",
    "\n",
    "            compression_model = custom_pca(ncomp)\n",
    "            compression_model.fit(all_frames)\n",
    "            frames_enc, shape = compression_model.encode(all_frames)\n",
    "            A = np.linalg.pinv(frames_enc[:-1])@frames_enc[1:]\n",
    "\n",
    "            X.append((compression_model, A))\n",
    "        print(y.shape, sum(mask_videos))\n",
    "        y = y[mask_videos]\n",
    "        print(y.shape)\n",
    "\n",
    "        # Compute all distances\n",
    "        full_martin_gram = np.zeros((len(X), len(X)))\n",
    "        for i, mi in enumerate(X):\n",
    "            for j, mj in enumerate(X):\n",
    "                if i > j:\n",
    "                    full_martin_gram[i,j] = full_martin_gram[j,i]\n",
    "                else:\n",
    "                    # Check: negative values appear when computing distance. For the moment round them to zero\n",
    "                    # while verifying they are all small enough\n",
    "                    martin_dist_curr = martin_dist(mi, mj)\n",
    "                    if martin_dist_curr < -1e-13:\n",
    "                        raise ValueError('Negative values should not appear.')\n",
    "\n",
    "                    full_martin_gram[i,j] = martin_dist_curr\n",
    "        full_martin_gram[full_martin_gram < 0] = 0\n",
    "\n",
    "        # Evaluation using KNN\n",
    "        confusions_baseline_knn = np.zeros((len(ks), ngestures, ngestures))\n",
    "        accuracies_baseline_knn = np.zeros(len(ks))\n",
    "        for ki, k in enumerate(ks):\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, metric='precomputed', n_jobs=-1)\n",
    "            accuracy, conf_matrix = evaluate_model(knn, full_martin_gram, y)\n",
    "            accuracies_baseline_knn[ki] = accuracy\n",
    "            confusions_baseline_knn[ki] = conf_matrix\n",
    "        all_confusions_baseline_knn[task].append(confusions_baseline_knn)\n",
    "        all_accuracies_baseline_knn[task].append(accuracies_baseline_knn)\n",
    "\n",
    "        # Evaluation using SVM\n",
    "        svm_model = SVC()\n",
    "        confusions_baseline_svm = np.zeros((len(gammas), ngestures, ngestures))\n",
    "        accuracies_baseline_svm = np.zeros(len(gammas))\n",
    "        for gammai, gamma in enumerate(gammas):\n",
    "            accuracy, conf_matrix = evaluate_model(svm_model, full_martin_gram, y, d_transform=lambda x: np.exp(-gamma*x))\n",
    "            accuracies_baseline_svm[gammai] = accuracy\n",
    "            confusions_baseline_svm[gammai] = conf_matrix\n",
    "        all_confusions_baseline_svm[task].append(confusions_baseline_svm)\n",
    "        all_accuracies_baseline_svm[task].append(accuracies_baseline_svm)\n",
    "\n",
    "        print('\\tSubject '+subject+': '+sec2string(time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
