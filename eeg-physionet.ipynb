{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strong-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from time import time, sleep\n",
    "\n",
    "from utils import *\n",
    "from jigsaws_utils import *\n",
    "from custom_pca import custom_pca\n",
    "from video_loader import VideoLoader\n",
    "from autoencoders import *\n",
    "from physionet_pytorch.utils.get_data import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=np.ComplexWarning)\n",
    "\n",
    "def cross_predict(model, gram_matrix, y, nsplits=5, d_transform=None):\n",
    "    nsample = len(y)\n",
    "    nsample_per_group = int(nsample/nsplits)\n",
    "    repeats = [nsample_per_group]*(nsplits-1)\n",
    "    repeats = repeats + [nsample-np.sum(repeats)]\n",
    "    groups = np.repeat(np.arange(nsplits), repeats)\n",
    "    kfolder = GroupKFold(n_splits=nsplits)\n",
    "\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    for i, (train_index, test_index) in enumerate(kfolder.split(y, groups=groups)):\n",
    "        if d_transform is not None:\n",
    "            gram_train = d_transform(gram_matrix[train_index, :][:, train_index])\n",
    "            gram_test = d_transform(gram_matrix[test_index, :][:, train_index])\n",
    "        else:\n",
    "            gram_train = gram_matrix[train_index, :][:, train_index]\n",
    "            gram_test = gram_matrix[test_index, :][:, train_index]\n",
    "\n",
    "        # Predict\n",
    "        model.fit(gram_train, y[train_index])\n",
    "        preds = model.predict(gram_test)\n",
    "        all_preds.append(preds)\n",
    "        all_true.append(y[test_index])\n",
    "\n",
    "    return np.concatenate(all_preds), np.concatenate(all_true)\n",
    "\n",
    "def evaluate_model(model, gram_matrix, y, gestures, nsplits=5, d_transform=None):\n",
    "    nsample = len(y)\n",
    "    nsample_per_group = int(nsample/nsplits)\n",
    "    repeats = [nsample_per_group]*(nsplits-1)\n",
    "    repeats = repeats + [nsample-np.sum(repeats)]\n",
    "    groups = np.repeat(np.arange(nsplits), repeats)\n",
    "    kfolder = GroupKFold(n_splits=nsplits)\n",
    "\n",
    "    ngestures = len(gestures)\n",
    "    confusion_matrix_cum = np.zeros((ngestures, ngestures))\n",
    "    count_gesture_presence = np.zeros((ngestures, ngestures))\n",
    "    accuracy_cum = 0\n",
    "    for train_index, test_index in kfolder.split(y, groups=groups):\n",
    "        if d_transform is not None:\n",
    "            gram_train = d_transform(gram_matrix[train_index, :][:, train_index])\n",
    "            gram_test = d_transform(gram_matrix[test_index, :][:, train_index])\n",
    "        else:\n",
    "            gram_train = gram_matrix[train_index, :][:, train_index]\n",
    "            gram_test = gram_matrix[test_index, :][:, train_index]\n",
    "\n",
    "        # Predict\n",
    "        model.fit(gram_train, y[train_index])\n",
    "        preds = model.predict(gram_test)\n",
    "        accuracy_cum += np.mean(y[test_index]==preds)\n",
    "        cm = confusion_matrix(y[test_index], preds, normalize='true')\n",
    "\n",
    "        # Handle of missing gestures\n",
    "        current_gesture_presence = np.ones(cm.shape)\n",
    "        if cm.shape != (ngestures,ngestures):\n",
    "            missing_gestures = np.setdiff1d(np.setdiff1d(gestures, preds), y[test_index])\n",
    "\n",
    "            print(gestures, missing_gestures)\n",
    "            for g in missing_gestures:\n",
    "                gi = g2i[g]\n",
    "                cm = np.insert(cm, gi, 0, axis=0)\n",
    "                cm = np.insert(cm, gi, 0, axis=1)\n",
    "                #cm[gi,gi] = 1\n",
    "                print('AAA')\n",
    "                print(current_gesture_presence)\n",
    "                current_gesture_presence = np.insert(current_gesture_presence, gi, 0, axis=0)\n",
    "                print('BBB')\n",
    "                print(current_gesture_presence)\n",
    "                current_gesture_presence = np.insert(current_gesture_presence, gi, 0, axis=1)\n",
    "                print('CCC')\n",
    "        confusion_matrix_cum += cm\n",
    "        count_gesture_presence += current_gesture_presence\n",
    "        print(current_gesture_presence)\n",
    "    #if np.any(count_gesture_presence==0):\n",
    "    #    print('In evaluate_model, zero count:')\n",
    "    print('End of splits')\n",
    "    print(count_gesture_presence)\n",
    "    average_confusion_matrix = confusion_matrix_cum/count_gesture_presence\n",
    "\n",
    "    accuracy = accuracy_cum / float(nsplits)\n",
    "    weighted_accuracy = np.mean(np.diag(average_confusion_matrix))\n",
    "\n",
    "    return accuracy, weighted_accuracy, average_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-catering",
   "metadata": {},
   "source": [
    "## Define initial values and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "owned-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['64_channel_sharbrough-old.png', '64_channel_sharbrough.pdf', '64_channel_sharbrough.png', 'ANNOTATORS', 'RECORDS', 'S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012', 'S013', 'S014', 'S015', 'S016', 'S017', 'S018', 'S019', 'S020', 'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S027', 'S028', 'S029', 'S030', 'S031', 'S032', 'S033', 'S034', 'S035', 'S036', 'S037', 'S038', 'S039', 'S040', 'S041', 'S042', 'S043', 'S044', 'S045', 'S046', 'S047', 'S048', 'S049', 'S050', 'S051', 'S052', 'S053', 'S054', 'S055', 'S056', 'S057', 'S058', 'S059', 'S060', 'S061', 'S062', 'S063', 'S064', 'S065', 'S066', 'S067', 'S068', 'S069', 'S070', 'S071', 'S072', 'S073', 'S074', 'S075', 'S076', 'S077', 'S078', 'S079', 'S080', 'S081', 'S082', 'S083', 'S084', 'S085', 'S086', 'S087', 'S088', 'S089', 'S090', 'S091', 'S092', 'S093', 'S094', 'S095', 'S096', 'S097', 'S098', 'S099', 'S100', 'S101', 'S102', 'S103', 'S104', 'S105', 'S106', 'S107', 'S108', 'S109', 'SHA256SUMS.txt', 'wfdbcal']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(sorted(os.listdir('data/physionet'))[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "duplicate-element",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning 2 Class data\n",
      "Data from runs: [4, 8, 12]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "data/physionet/S001/S001R04.edf: the file is not EDF(+) or BDF(+) compliant (Prefilter)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ba53206b3629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mncomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/physionet/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/documents/epfl/ma4/tic-detection/physionet_pytorch/utils/get_data.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(path, long, normalization, subjects_list, n_classes)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Data from runs: {mi_runs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubjects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmi_runs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# do normalization if wanted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/documents/epfl/ma4/tic-detection/physionet_pytorch/utils/get_data.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(subjects, runs, path, long)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m# Read file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;31m# Signal Parameters - measurement frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSampleFrequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpyedflib/_extensions/_pyedflib.pyx\u001b[0m in \u001b[0;36mpyedflib._extensions._pyedflib.CyEdfReader.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpyedflib/_extensions/_pyedflib.pyx\u001b[0m in \u001b[0;36mpyedflib._extensions._pyedflib.CyEdfReader.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpyedflib/_extensions/_pyedflib.pyx\u001b[0m in \u001b[0;36mpyedflib._extensions._pyedflib.CyEdfReader.check_open_ok\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: data/physionet/S001/S001R04.edf: the file is not EDF(+) or BDF(+) compliant (Prefilter)"
     ]
    }
   ],
   "source": [
    "ks = np.arange(1, 8)\n",
    "gammas = np.logspace(-10, -5, base=2, num=10)\n",
    "ncomp = 10\n",
    "\n",
    "X,y = get_data('data/physionet/', n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "therapeutic-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/jules/documents/epfl/ma4/tic-detection/data/physionet/S004/S004R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /home/jules/documents/epfl/ma4/tic-detection/data/physionet/S004/S004R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /home/jules/documents/epfl/ma4/tic-detection/data/physionet/S004/S004R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "<class 'mne.io.edf.edf.RawEDF'>\n",
      "saving to data_generated/test.S4R061014.edf, filetype 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mne.datasets import eegbci\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "import pyedflib as edf\n",
    "from save_edf import write_mne_edf\n",
    "\n",
    "#Define the parameters \n",
    "subject = 4  # use data from subject 1\n",
    "runs = [6, 10, 14]  # use only hand and feet motor imagery runs\n",
    "\n",
    "files = [f'data/physionet/S{subject:03}/S{subject:03}R{r:02}.edf' for r in runs]\n",
    "#Read raw data files where each file contains a run\n",
    "raws = [read_raw_edf(f, preload=True) for f in files]\n",
    "#Combine all loaded runs\n",
    "raw_obj = concatenate_raws(raws)\n",
    "print(type(raw_obj))\n",
    "\n",
    "gen_file = 'data_generated/test.S4R061014.edf'\n",
    "write_mne_edf(raw_obj, 'data_generated/test.S4R061014.edf', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tender-stake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq: 160\n",
      "Number of channels: 64\n"
     ]
    }
   ],
   "source": [
    "er = edf.EdfReader(gen_file)\n",
    "fs = er.getSampleFrequency(0)\n",
    "n_ch = er.signals_in_file\n",
    "er.close()\n",
    "\n",
    "print('Freq:', fs)\n",
    "print('Number of channels:', n_ch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
