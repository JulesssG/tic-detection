{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Device: Tesla P100-PCIE-12GB\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from time import time, sleep\n",
    "\n",
    "from utils import *\n",
    "from custom_pca import custom_pca\n",
    "from video_loader import VideoLoader\n",
    "from autoencoders import *\n",
    "\n",
    "seed = 42\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print('Device:',torch.cuda.get_device_name(device))\n",
    "models = torch.load('data_generated/week7/models.pth', map_location=device)\n",
    "\n",
    "# Convert TempConvAE's weights\n",
    "d = models['10-TempConvAE']['model']\n",
    "d[\"to_lower_rep.weight\"] = d[\"low_dim_mapping.0.weight\"]\n",
    "d[\"to_lower_rep.bias\"] =   d[\"low_dim_mapping.0.bias\"]\n",
    "d[\"from_lower_rep.weight\"] = d[\"low_dim_mapping.1.weight\"]\n",
    "d[\"from_lower_rep.bias\"] = d[\"low_dim_mapping.1.bias\"]\n",
    "del d[\"low_dim_mapping.0.weight\"]\n",
    "del d[\"low_dim_mapping.0.bias\"]\n",
    "del d['low_dim_mapping.1.weight']\n",
    "del d['low_dim_mapping.1.bias']\n",
    "del models['10-OneHAE']['little_modification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2task = {0: 'Knot_Tying',\n",
    "        1: 'Needle_Passing',\n",
    "        2: 'Suturing'}\n",
    "ntask = len(i2task.keys())\n",
    "task2i = {v: k for k, v in i2task.items()}\n",
    "\n",
    "# Gesture index to description\n",
    "gi2descr = {\n",
    "    1: 'Reaching for needle with right hand',\n",
    "    2: 'Positioning needle',\n",
    "    3: 'Pushing needle through tissue',\n",
    "    4: 'Transferring needle from left to right',\n",
    "    5: 'Moving to center with needle in grip',\n",
    "    6: 'Pulling suture with left hand',\n",
    "    7: 'Pulling suture with right hand',\n",
    "    8: 'Orienting needle',\n",
    "    9: 'Using right hand to help tighten suture',\n",
    "    10: 'Loosening more suture',\n",
    "    11: 'Dropping suture at end and moving to end points',\n",
    "    12: 'Reaching for needle with left hand',\n",
    "    13: 'Making C loop around right hand',\n",
    "    14: 'Reaching for suture with right hand',\n",
    "    15: 'Pulling suture with both hands'\n",
    "}\n",
    "\n",
    "def load_video_data(tasks=None, subjects=None, trials=None, captures=None, gestures=None):\n",
    "    if tasks is None:\n",
    "        tasks = np.array(list(task2i.values()))\n",
    "    else:\n",
    "        tasks = np.array(tasks).ravel()\n",
    "\n",
    "    root_path = 'data/JIGSAWS_converted'\n",
    "    #or_tasks = '\\|'.join([i2task[task] for task in tasks])\n",
    "    video_meta = !find $root_path -name '*.avi' | sed 's:^.*/\\([^/]\\+_[A-Z][0-9]\\{3\\}\\)_.*$:\\1:'\n",
    "    video_meta = [name[-4:] for name in video_meta for task in tasks if i2task[task] in name]\n",
    "    if subjects is None:\n",
    "        subjects = np.unique([x[0] for x in video_meta])\n",
    "    else:\n",
    "        subjects = np.array(subjects).ravel()\n",
    "\n",
    "    if trials is None:\n",
    "        trials = np.unique([x[-1] for x in video_meta])\n",
    "    else:\n",
    "        trials = np.array(trials).ravel()\n",
    "\n",
    "    if captures is None:\n",
    "        captures = np.array([1,2])\n",
    "    else:\n",
    "        captures = np.array(captures).ravel()\n",
    "\n",
    "    if gestures is None:\n",
    "        gestures = np.array(list(gi2descr.keys()))\n",
    "    else:\n",
    "        gestures = np.array(gestures).ravel()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    for task in tasks:\n",
    "        task_name = i2task[task]\n",
    "        for subject in subjects:\n",
    "            for trial in trials:\n",
    "                transcr_filename = f'{root_path}/{task_name}/transcriptions/{task_name}_{subject}00{trial}.txt'\n",
    "                try:\n",
    "                    with open(transcr_filename, 'r') as fp:\n",
    "                        for l in fp.readlines():\n",
    "                            start_frame, end_frame, gesture = l.split()\n",
    "                            start_frame = int(start_frame)\n",
    "                            end_frame = int(end_frame)\n",
    "                            gesture = int(gesture[1:])\n",
    "                            if not gesture in gestures:\n",
    "                                continue\n",
    "                            for capt in [1,2]:\n",
    "                                video_filename = f'{root_path}/{task_name}/video/{task_name}_{subject}00{trial}_capture{capt}.avi'\n",
    "                                fragment = VideoLoader(video_filename, gray=True, start_frame=start_frame,\n",
    "                                                       duration_frames=end_frame-start_frame+1)\n",
    "                                fragment.trial = trial\n",
    "                                fragment.jig_capture = capt\n",
    "                                fragment.subject = subject\n",
    "                                fragment.task = task\n",
    "\n",
    "                                X.append(fragment)\n",
    "                                y.append(gesture)\n",
    "                except FileNotFoundError:\n",
    "                    print(f'Video not present: task {task_name}, subject {subject}, trial {trial}, capture {capt}')\n",
    "                    break\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification procedure for participant B, Suturing task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp = 10\n",
    "task = 2\n",
    "subject = 'B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points creation for baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data, y = load_video_data(tasks=task, subjects=subject)\n",
    "nsample = len(y)\n",
    "X = []\n",
    "for video in video_data:\n",
    "    video.torch = False\n",
    "    all_frames = video.get_all_frames().reshape(video.duration_frames, -1)\n",
    "\n",
    "    compression_model = custom_pca(ncomp)\n",
    "    compression_model.fit(all_frames)\n",
    "    frames_enc, shape = compression_model.encode(all_frames)\n",
    "    A = np.linalg.pinv(frames_enc[:-1])@frames_enc[1:]\n",
    "\n",
    "    X.append((compression_model, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of all distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_martin_gram = np.zeros((len(X), len(X)))\n",
    "for i, mi in enumerate(X):\n",
    "    for j, mj in enumerate(X):\n",
    "        if i > j:\n",
    "            full_martin_gram[i,j] = full_martin_gram[j,i]\n",
    "        else:\n",
    "            full_martin_gram[i,j] = martin_dist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
