Semester project info:
- Title: Learning the Dynamics of Complex and High-dimensional Time-series (too vague)
- Teacher: Benjamin Bejar Haro
- Assistant: ???
- Keywords: Tic disorder, Linear Dynamical Systems, Auto-encoder, Deep-learning, Image Stream processin, Activity recognition
- Student's comment:

Week -1 (1.08 - 8.09):
Asked to begin the setup of the environment for a basic LDS framework that uses PCA to go from signal space to low-dimensional representation. Because of the very high-dimensionnality of the signal -> incrementalPCA from sklearn. For this setup, need to create a class that load portions of video (chunks composed of a small numbers of frames) and convert it to tensor (so that it can be feed to the PCA model).

I dived into the Doretto's paper to understand how we will use the LDS formalism here. I focused on the loading of the video as it is the main challenge in terms of code for this part (at least until the training of the transition matrix for the low-dimensional representation of the signal). Problem: can't even load more than 1 minute of video without it to fill all my 20GB of RAM. Possible solutions I look into are preprocess the video into chunks converted to tensor written to disk and find a way to load a portion of video, convert it, feed it to PCA, free it, repeat until a PCA model for each video has been created so that we can convert this video.

Goal until the meeting with all the team on this Thursday: find how we will load those videos and code an example of loading -> create PCA -> convert it -> write it to see how that affect the video on a small clip (10-20 sec).

Week 0 (8.09 - 15.09):
- Done the loading -> PCA -> inverse_PCA -> writting for 20s of grayscale video with quality 960x540. Cannot be done in one go for the same video in color.
- Added VideoIterator that permit to iterate over the video so that we can use partial_fit of IncrementalPCA. This avoided the RAM to overload. This permits to transform the colored video in 4 times more time than the gray scale (3 times more dimensions so it seems to scale pretty well).

- Bottleneck is clearly the PCA fitting time, may not be normal tthat it takes so long -> comparison to SVD. Problem is that the SVD cannot be computed iteratively (no?). Tried to load a video of 22min, without feeding it to PCA to see if it scales and yes it works.

Week 1:
Goals of the week:
- Better comparison of the fitting time of PCA between iterative and all at once
- Find why the video is saturated after PCA compression and correct it
- Write  a function that scales on video to asked quality (will be useful later to reduce dim)
- Find a way to go from SVD to a PCA model so that with an SVD we can reduce the dim (and back)
- Adapt the SVD decomposition so that it can be computed to longer video (take N random frames with N fixed no matter the video duration to estimate the SVD)
- Compare quality of reconstruction between approximation of the SVD and exact SVD (and PCA?)

Comparison of PCA fitting time:
Loading and fitting time fro grayscale, 960x540, 20s video reduced to 20 components:
- All at once: 2m6s
- Iterator: 2m18s

TAG: week0

- Corrected saturation
- Scaling included in the VideoLoader contructor (optional argument)
- PCA using svd done with the custom_pca class
- For longer video, read_video (with the random argument) can be used to load a given amount of random frames from a video. That can be used to fit the custom PCA model and then the whole video can be converted using this object. (Not in this function anymore, directly in the VideoLoader)
- Comparison for quality of reconstruction and computational time for 20s 960x540 (color):

Model:			Custom PCA	Custom PCA	Custom PCA	Custom PCA	Custom PCA	PCA_64		PCA_128
Frames:			100%				50%					25%					10%					5%					100%			100%
Fit time:		1m4s				37s					15s					7s					5.9s				5m32s			6m22s
Trans time:	42s					15s					15s					15s					15s					32s				39s
l2-error:		8.5					41.6				48.2				49.2, 73		271, 93			32.8			32.8

So the loss of the first half of frame hurt, but after it's mostly the variance that rises. SVD is way faster than PCA, and gives better result when using all frames (the partial fitting is not equivalent to a full PCA model).
Also, the transformation is faster when done in batches.

Meeting re-scheduled
Registration to project's done (Assistant: None)

TAG: week1

Week 2:
Goals:
- Write a framework for LDS and see how to optimize it
- Make more experiments as to see how the reconstruction error evolve with the number of components

5 minutes, grayscale, 256x256, 0.5 frames to fit:
ncomp					10		25		50		100		150
fit time			30.8	33.4	36		50.8	72.3
trans time		10.9	11.1	12.5	14.4	18.3
error					8.45	7.98	7.45	6.71	6.16

10 minutes, grayscale, 256x256, 0.2 frames to fit:
ncomp					10		25		50		100		150
fit time			13		13		17		22		32
trans time		21		23		26		32		35
error					8.62	8.1		7.59	6.85	6.31

10 minutes, grayscale, 256x256 (scaled at runtime), 0.2 frames to fit:
ncomp					10		25		50		100		150
fit time			44		44		47		51		59
trans time		87		80		86		89		89
error					8.63	8.13	7.62	6.85	6.33

10 minutes, grayscale, 256x256, 0.5 to fit:
ncomp					10		25		50		100		150
fit time			28		27		30		45		1m5s
trans time		22		19		22		27		30
error					8.61	8.1		7.58	6.8		6.25

LDS done using torch. Optimize the l2 prediction error. This permit to predict the whole video using only the pca model, the lds model and the low-rank representation of the starting frame. This gives:
- Reconstruction error of low-dim frames: 2.26
- Reconstruction error of generated frames: 9.48

TAG: week2

Week 3:
Goals:
- Write an autoencoder to improve the reconstruction error, pca will be the baseline

Written an autoencoder that can map 256x256 tensors to the following dimensions: 16, 25, 50, 100, 150, 200

Re-written the autoencoder to reduce the number of parameters by a factor of 4, now either the model has not enough capacity, either the stopping of the optimization is too early. The latter is much more probable as I only let it run for 3 epoch (the fitting take ~2min per epoch). The structure of the net is as follows:
BasicAutoEncoder(
  (transform_convs): Sequential(
    (0): Conv2d(1, 8, kernel_size=(4, 4), stride=(1, 1))									-> 8 x 253 x 253
    (1): ReLU()
    (2): Conv2d(8, 16, kernel_size=(6, 6), stride=(3, 3), padding=(1, 1))	-> 16 x 84 x 84
    (3): ReLU()
    (4): Conv2d(16, 16, kernel_size=(6, 6), stride=(2, 2))								-> 16 x 40 x 40
    (5): ReLU()
    (6): Conv2d(16, 16, kernel_size=(6, 6), stride=(2, 2))								-> 16 x 18 x 18
    (7): ReLU()
    (8): Conv2d(16, 8, kernel_size=(6, 6), stride=(1, 1))									-> 8 x 13 x 13
    (9): ReLU()
  )
  (to_lower_dim): Sequential(
    (0): Conv2d(8, 1, kernel_size=(7, 7), stride=(2, 2))									-> 1 x 4 x 4
    (1): ReLU()
  )
  (from_lower_dim): Sequential(
    (0): ConvTranspose2d(1, 8, kernel_size=(7, 7), stride=(2, 2))
    (1): ReLU()
  )
  (inv_transform_convs): Sequential(
    (0): ConvTranspose2d(8, 16, kernel_size=(6, 6), stride=(1, 1))
    (1): ReLU()
    (2): ConvTranspose2d(16, 16, kernel_size=(6, 6), stride=(2, 2))
    (3): ReLU()
    (4): ConvTranspose2d(16, 16, kernel_size=(6, 6), stride=(2, 2))
    (5): ReLU()
    (6): ConvTranspose2d(16, 8, kernel_size=(6, 6), stride=(3, 3), padding=(1, 1))
    (7): ReLU()
    (8): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(1, 1))
  )
)

After five epochs (5min of grayscale 256x256, skipping 4 frames with five epochs -> 5min30 to fit and 2min to transform) the error is quite high: 40.39. One problem is that the frame are not at all extracted in random order but in consecutive which implies that the SGD's convergence rate should be much worse.

Added the randit VideoLoader's argument to make the iterator load frames in random order.

Current state of VideoLoader:
###		arguments		###
filename (string): the filename of the video
duration (number): the duration (in seconds) that will be loaded
batch_size (int): the number of frames returned for every iteration of the iterator
grayscale (boolean): whether to convert the video to grayscale or not
scale ((int, int)): whether to scale the video resolution or not, it will transform whatever resolution of the video file into scale[0] x scale[1]
skip_frame (int):	Used in the iterator only, how many consecutive frames to skip after every loaded frame
randit (boolean): Used only in the iterator

###		functions		###
get_all_frames:
	argument: -
	dependencies: filename, fps, duration, scale, grayscale

	return all frames in consecutive order up to the duration

get_random_frames:
	argument:
		frames_ratio (float): coefficient between 0 and 1 representing the proportion of frames that will be loaded
		seed: random seed for reproducibility
		dependencies: filename, fps, scale, grayscale (does not depend on duration!)

		return total_frames*frames_ratio frames in random order

frame_transform:

	transform the frames according to the scale and the grayscale arguments

###		iterator		###
	Iterator over the frames of the video present in filename. It will at each iteration return batch_size frames, transformed using frame_transformed. The number of frames return in total (after the StopIteration()) will be duration*fps/(skip_frame+1). This skip_frame argument is mainly here when loading in consecutive order, to load only 1 frame every skip_frame+1 frames.

Using random order when loading the error goes down to 36.04.

TAG: no tag because no real results

Week 4:
Problems with last week's work: should have started with a simple model and improve it step by step. The complicated autoencoder written is not really useable.
Goals:
- Implement PCA using an autoencoder and see if we can get a network that performs around the same as PCA
- Then, improve it using non-linearities, more layers, convolutions etc.

Implemented PCA using autoencoder structure. Far slower to create the model and the performance are worse (normal as PCA is mathematically perfect). After tuning the learning rate and the weight decay, fitting for 30 epochs gives an error of 17.8 (vs 7.45 for PCA). This took 4m43s to fit and 24s to transform (vs 36s and 12.5s for PCA). So for now this model is just worse in all aspect (except maybe that it can be fitted iteratively).

Next model is a one hidden layer NN for the encoder and decoder with 200 hidden dimension. This gives a slightly better result: 14.62 (same as before, hyper-parameters tuned ans 30 epochs) but take 2x more times per epoch.

Implemented convolutional AE with few layer and big filters. Good potential but slow to converge.
Model:
SpatialConvAE(
  (encoder_convs): Sequential(
    (0): Conv2d(1, 128, kernel_size=(26, 26), stride=(5, 5))
    (1): ReLU()
    (2): Conv2d(128, 128, kernel_size=(11, 11), stride=(3, 3))
    (3): ReLU()
    (4): Conv2d(128, 64, kernel_size=(6, 6), stride=(1, 1))
    (5): ReLU()
  )
  (encoder_lin): Linear(in_features=4096, out_features=50, bias=True)
  (decoder_lin): Linear(in_features=50, out_features=4096, bias=True)
  (decoder_convs): Sequential(
    (0): ConvTranspose2d(64, 128, kernel_size=(6, 6), stride=(1, 1))
    (1): ReLU()
    (2): ConvTranspose2d(128, 128, kernel_size=(11, 11), stride=(3, 3))
    (3): ReLU()
    (4): ConvTranspose2d(128, 1, kernel_size=(26, 26), stride=(5, 5))
  )
)
Using 30 epochs, fine tuned the hyperparameters:
	time to fit: 1:15:04
	time to transform: 3:46
	error: around 14 (exact loss not known, kernel died)

Switch to video that have a meaning: one recorded by the Jo's. The aim is to make a fresh comparison of all models and try to come with an autoencoder that compete with PCA.

Had to debug the video loader while switching the recording. The video loader is now faster (as fast as before including torch data types into the video loader).

The PCA autoencoder is hard to train, learning rate and weight decay have practically no effect, from 1e-6 to 1e-3 the learning rate is pretty much the same and it saturate at ~150. However, the variation of the error between one optimisation and the other (same parameters and data, just different randomness for the weight initialization etc.) is very high. It seems that the model get stucks in the majority of time into local minimas.

For the autoencoder with one hidden layer, it works way better. It achieves 17 error after 3m31s (40 epochs).

The spatial convolutional AE still is very slow, but seems to converge towards a great approximation.

TAG: week4

Week 5:
Temporal convolution should be the way to go so the goal is to one step at a time create such a model.

Switched to colab to run the experiment on temporal convolutions. Tried 1 to 5 layers for temporal convolutions with stride 2, kernel size 8x8x8, channels 4,8,12,32 and 64.

Conclusions:
- Testing time grows linearly for 2 channels and more (30s for 10s of video with 32 channels)
- From 2 layers, curves looks roughly the same. Increasing the number of layers makes the training harder and augmenting the number of channels improve the performance.
- 2 channels and 32 channels seems like a good compromise.

TAG: week5

Week 6 and 7:	(Laptop died during the week 6)
Remarks on temporal convolutions so far:
- May be better to combine spatial convs + LSTM (see 2016-ICLR-video-autoencoder's structure)
- More than 2 seconds as batch size to predict the next frame does not make much sense, 32 as maximum maybe
- Batches are disjoint. Frame N to N+B are there to predict the frame N+B+1 alone, we use each frame exactly one time construct the low dim. rep. for 1/B of all frames only.
- Because of the last bullet point, at test time to try to predict each frame we have to run the temporal convolution for the chosen time interval for each frame. Meaning, for the current setup (64 frames as input to the model) to predict each frame the model has to process 64 frames, which is really time consuming.
- Another disadvantage of 64 frames as input for the model is that the bottleneck of the autoencoder (the low dimensional representation) must contain information relative to all those frames, even tho they are not all useful.
- The argument to iterate in a random order was True for all experiments, this does not make much sense for our application, as the temporal convolutions are there to capture temporal dependencies.
- The training may benefit (in the future) from a better loss function. With for example the gradient of the image to preserve edges.

Added the possibility to configure the stride when iterating on the video. This facilitates the iteration for temporal convolutions.

Added criterion (not using it yet) that combine the mse with the gradient, to preserve the edges.

Tried to add a one hidden fc layer at the end of the encoder and beginning of decoder, did not seem to improve the performance of the model.

Compared all models' reconstruction error and save figures and models. The convolutional network and the OneH autoencoder have some potential but the PCA is still far ahead. Compared how the convolutional model behave when we force its latent dimension (mapping with linear layer).

TAG: week7

Week 8:
Goals:
- Incorporate dynamical system's prediction of frame to pipeline and minimze the prediction
- See technical_notes.pdf for precisions (https://www.overleaf.com/project/5fa510249465963b8b085f0b)

Midterm presentation: see slides and repo for details
TAG: week8, midterm

Week 9:
The videos from Joey and Joseph are not yet available so for the moment we'll use another dataset to test the models, namely JIGSAWS (https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/). Next steps are:
- Look into the UNet autoencoder, to have a model proven to work to compare (and use as is if it gives good results).
- Implement the classification part based on the transition matrix.

Implemented the Martin and Frobenius distances using the subspace angles between the transition matrices. For now to test, just fit all videos separately and plot the distances between the video's models for all activity combinations.

TAG: week9

Week 10:
- Take a look at UNet and add it to the models
- Correct the dataset loading (was taking task as activities)
- Correct the distances function -> without cos

Obtained classification results for subject B with Suturing task. Depending on the k used in nearest-neighbors the classification's accuracy is 0.8-0.95 for separated learning. 
The PCA autoencoder with proper initialization performs better than the original PCA with accuracy between 0.8-0.9 and with more stable accuracy across k's. 
The performance of the one hidden layer autoencoder is terrible with an accuracy of random guessing pretty much.

TAG week10

Week 11:
Switch from initial goal -> focus on linear (but joint) learning
(paper for automatic segmentation: https://www.biorxiv.org/content/10.1101/2020.10.15.341602v1.full.pdf)
- For one gesture in a trial, compute models for each fragment of this gesture, plot the prediction error wrt the frames. We should have a lower error when the frames are in the same gesture and a even lower when the frames were used for the models.
- Train a classifier f: R^{nf} -> {0,1} with nf the number of fragement for the given gesture. This classifier predict if the corresponding frame (a vector of nf values corresponding the prediction error for each model) is of the gesture or not.
- Adapt the loss to minimize a loss enforcing the models of same gesture to be clustered (see math notes)
- Use SVM (precomputed kernel in SVC) to classify to see if it improves the performance
