{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "from time import time, sleep\n",
    "\n",
    "from utils import *\n",
    "from custom_pca import custom_pca\n",
    "from video_loader import VideoLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Example of video transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = custom_pca()\n",
    "\n",
    "video = VideoLoader('data/sample20s.mp4', grayscale=True)\n",
    "t1 = time()\n",
    "frames_rand = video.get_random_frames(0.6)\n",
    "model.fit(frames_rand)\n",
    "t2 = time()\n",
    "reconstructed = []\n",
    "for j, frames in enumerate(video):\n",
    "    reconstructed.append(model.inverse_transform(model.transform(frames), shape=(video.height, video.width)))\n",
    "reconstructed = np.vstack(reconstructed)\n",
    "t3 = time()\n",
    "print(reconstruction_error(video.get_all_frames(), reconstructed))\n",
    "t4 = time()\n",
    "\n",
    "print('Fitting time:', t2-t1)\n",
    "print('Transform:', t3-t2)\n",
    "print('Error calculation:', t4-t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Dimensionality reduction using autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16])\n",
      "torch.Size([64, 32, 256, 256])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 32, 256, 256])\n",
      "torch.Size([64, 50])\n",
      "torch.Size([64, 32, 256, 256])\n",
      "torch.Size([64, 100])\n",
      "torch.Size([64, 32, 256, 256])\n",
      "torch.Size([64, 150])\n",
      "torch.Size([64, 32, 256, 256])\n",
      "torch.Size([64, 200])\n",
      "torch.Size([64, 32, 208, 208])\n",
      "Number of parameters for 200: 193000\n"
     ]
    }
   ],
   "source": [
    "class BasicAutoEncoder(nn.Module):\n",
    "    def __init__(self, inchannels, ncomp):\n",
    "        super().__init__()\n",
    "        \n",
    "        # After that: B x 16 x 13 x 13\n",
    "        self.transform_convs = nn.Sequential(nn.Conv2d(inchannels, 32, kernel_size=4), nn.ReLU(), # 253\n",
    "                                            nn.Conv2d(32, 32, kernel_size=5, stride=2), nn.ReLU(), # 125\n",
    "                                            nn.Conv2d(32, 32, kernel_size=5, stride=2), nn.ReLU(), # 61\n",
    "                                            nn.Conv2d(32, 32, kernel_size=5), nn.ReLU(), # 57\n",
    "                                            nn.Conv2d(32, 16, kernel_size=3, stride=2), nn.ReLU(), # 28\n",
    "                                            nn.Conv2d(16, 16, kernel_size=4, stride=2), nn.ReLU()) # 13\n",
    "        \n",
    "        \"\"\"\n",
    "            to_lower_dim: map each sample from 16 x 12 x 12 to ncomp dimensions\n",
    "            from_lower_dim: does the inverse mapping\n",
    "        \"\"\"\n",
    "        self.ncomp = ncomp\n",
    "        if ncomp == 200:\n",
    "            self.to_lower_dim = nn.Sequential(nn.Conv2d(16, 8, kernel_size=5, stride=2), nn.ReLU())\n",
    "            self.from_lower_dim = nn.Sequential(nn.ConvTranspose2d(8, 16, kernel_size=2, stride=2), nn.ReLU())\n",
    "        elif ncomp == 150:\n",
    "            self.to_lower_dim = nn.Sequential(nn.Conv2d(16, 6, kernel_size=5, stride=2), nn.ReLU())\n",
    "            self.from_lower_dim = nn.Sequential(nn.ConvTranspose2d(6, 16, kernel_size=5, stride=2), nn.ReLU())\n",
    "        elif ncomp == 100:\n",
    "            self.to_lower_dim = nn.Sequential(nn.Conv2d(16, 4, kernel_size=5, stride=2), nn.ReLU())\n",
    "            self.from_lower_dim = nn.Sequential(nn.ConvTranspose2d(4, 16, kernel_size=5, stride=2), nn.ReLU())\n",
    "        elif ncomp == 50:\n",
    "            self.to_lower_dim = nn.Sequential(nn.Conv2d(16, 2, kernel_size=5, stride=2), nn.ReLU())\n",
    "            self.from_lower_dim = nn.Sequential(nn.ConvTranspose2d(2, 16, kernel_size=5, stride=2), nn.ReLU())\n",
    "        elif ncomp == 25:\n",
    "            self.to_lower_dim = nn.Sequential(nn.Conv2d(16, 1, kernel_size=5, stride=2), nn.ReLU())\n",
    "            self.from_lower_dim = nn.Sequential(nn.ConvTranspose2d(1, 16, kernel_size=5, stride=2), nn.ReLU())\n",
    "        elif ncomp == 16:\n",
    "            self.to_lower_dim = nn.Sequential(nn.Conv2d(16, 1, kernel_size=7, stride=2), nn.ReLU())\n",
    "            self.from_lower_dim = nn.Sequential(nn.ConvTranspose2d(1, 16, kernel_size=7, stride=2), nn.ReLU())\n",
    "        else:\n",
    "            print('The lower dimension must be one of:', self.ncomps())\n",
    "            return\n",
    "            \n",
    "        # Inverse of the transform's convolutions\n",
    "        self.inv_transform_convs = nn.Sequential(nn.ConvTranspose2d(16, 16, kernel_size=4, stride=2), nn.ReLU(),\n",
    "                                                 nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2), nn.ReLU(),\n",
    "                                                 nn.ConvTranspose2d(32, 32, kernel_size=5), nn.ReLU(),\n",
    "                                                 nn.ConvTranspose2d(32, 32, kernel_size=5, stride=2), nn.ReLU(),\n",
    "                                                 nn.ConvTranspose2d(32, 32, kernel_size=5, stride=2), nn.ReLU(),\n",
    "                                                 nn.ConvTranspose2d(32, 32, kernel_size=4), nn.ReLU())\n",
    "        \n",
    "    def transform(self, x):\n",
    "        x = self.transform_convs(x)\n",
    "        x = self.to_lower_dim(x)\n",
    "        \n",
    "        return x.view(x.shape[0], -1), x.shape\n",
    "    \n",
    "    def inverse_transform(self, x, shape):\n",
    "        x = x.view(shape)\n",
    "        x = self.from_lower_dim(x)\n",
    "        x = self.inv_transform_convs(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def ncomps():\n",
    "        return [16, 25, 50, 100, 150, 200]\n",
    "    \n",
    "x = torch.normal(0, 5, (64, 3, 256, 256))\n",
    "for ncomp in BasicAutoEncoder.ncomps():\n",
    "    network = BasicAutoEncoder(3, ncomp)\n",
    "    x_low, x_shape = network.transform(x)\n",
    "    print(x_low.shape)\n",
    "    x_reconstructed = network.inverse_transform(x_low, x_shape)\n",
    "    print(x_reconstructed.shape)\n",
    "print(f'Number of parameters for {network.ncomp}:', sum([p.numel() for p in network.parameters()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
